# LeetCode AI Agent

A FastAPI-based AI assistant that helps users debug LeetCode problems using Ollama LLM and PostgreSQL with vector embeddings.

## ğŸš€ Quick Start

### Prerequisites

- Docker and Docker Compose
- Python 3.8+
- Ollama
- Git

### 1. Clone and Setup

```bash
git clone <your-repo-url>
cd agents
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Start All Services

```bash
# Start database and cache
make up

# Run database migrations
make migrate

# Seed database with sample data
make seed

# Generate embeddings (requires Ollama)
make embed

# Start the FastAPI server
make run
```

### 3. Start Ollama (in a separate terminal)

```bash
# Start Ollama server
ollama serve

# Pull required models (if not already downloaded)
ollama pull qwen2.5:7b-instruct
ollama pull mxbai-embed-large
```

## ğŸ“‹ Available Commands

| Command | Description |
|---------|-------------|
| `make up` | Start PostgreSQL and Redis containers |
| `make down` | Stop all containers |
| `make migrate` | Run database migrations |
| `make seed` | Load sample problem data |
| `make embed` | Generate embeddings for problem chunks |
| `make run` | Start FastAPI server |

## ğŸŒ API Endpoints

- **API Server**: http://localhost:8001
- **API Documentation**: http://localhost:8001/docs
- **Chat Endpoint**: `POST /chat/`

### Example API Usage

```bash
curl -X POST "http://localhost:8001/chat/" \
  -H "Content-Type: application/json" \
  -d '{
    "conversation_id": "test-conversation",
    "user_message": "I am getting a runtime error in my two sum solution. Can you help me debug it?",
    "problem_id": "8e9c2b58-3c0b-4c7a-8a8f-2f2e7fd0a01a",
    "lang": "python",
    "run_summary": "IndexError: list index out of range"
  }'
```

## ğŸ—ï¸ Architecture

### Services

- **FastAPI Server** (Port 8001): Main API server
- **PostgreSQL** (Port 5432): Database with pgvector extension
- **Redis** (Port 6379): Caching layer
- **Ollama** (Port 11434): LLM server

### Models

- **qwen2.5:7b-instruct**: Main language model for chat responses
- **mxbai-embed-large**: Embedding model for vector search

### Database Schema

- `problems`: LeetCode problem definitions
- `problem_chunks`: Text chunks with embeddings for retrieval
- `language_scaffolds`: Code templates for different languages
- `test_cases`: Public and private test cases
- `conversations`: Chat history (stored in Redis cache)

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file in the project root:

```bash
# Database
PG_DSN=postgresql://leet:coach@localhost:5432/leetcode

# Ollama
OLLAMA_BASE_URL=http://localhost:11434
LLM_MODEL=qwen2.5:7b-instruct
OLLAMA_EMBED_MODEL=mxbai-embed-large

# Memory
MEMORY_PATH=./memory/chat_history.json
```

### Docker Services

The `docker-compose.yml` defines:
- PostgreSQL with pgvector extension
- Redis for caching
- Persistent volumes for data

## ğŸ› Troubleshooting

### Common Issues

1. **Port conflicts**: Make sure ports 5432, 6379, 8001, and 11434 are available
2. **Ollama not responding**: Ensure Ollama is running with `ollama serve`
3. **Database connection errors**: Check if PostgreSQL container is running with `docker ps`
4. **Missing models**: Pull required models with `ollama pull <model-name>`

### Service Status Check

```bash
# Check all services
curl -s http://localhost:8001/docs > /dev/null && echo "âœ… FastAPI Running" || echo "âŒ FastAPI Down"
curl -s http://localhost:11434/api/tags > /dev/null && echo "âœ… Ollama Running" || echo "âŒ Ollama Down"
docker ps | grep leetcoach-db && echo "âœ… PostgreSQL Running" || echo "âŒ PostgreSQL Down"
docker ps | grep leetcoach-redis && echo "âœ… Redis Running" || echo "âŒ Redis Down"
```

### Reset Everything

```bash
# Stop all services
make down

# Remove volumes (WARNING: This deletes all data)
docker volume rm agents_dbdata

# Start fresh
make up
make migrate
make seed
make embed
```

## ğŸ“ Project Structure

```
agents/
â”œâ”€â”€ api/                    # FastAPI application
â”‚   â”œâ”€â”€ main.py            # Main API server
â”‚   â””â”€â”€ routes/            # API route handlers
â”œâ”€â”€ core/                  # Core business logic
â”‚   â”œâ”€â”€ llm_agent.py       # LLM interaction
â”‚   â”œâ”€â”€ memory.py          # Memory management
â”‚   â”œâ”€â”€ prompts.py         # Prompt templates
â”‚   â””â”€â”€ retrieval.py        # Vector search
â”œâ”€â”€ db/                    # Database utilities
â”‚   â”œâ”€â”€ init_schema.sql    # Database schema
â”‚   â”œâ”€â”€ seed.py            # Data seeding
â”‚   â”œâ”€â”€ embed_chunks.py    # Embedding generation
â”‚   â””â”€â”€ utils.py           # DB utilities
â”œâ”€â”€ data/                  # Sample data
â”‚   â””â”€â”€ two_sum.json       # Two Sum problem data
â”œâ”€â”€ memory/                # Cache storage
â”œâ”€â”€ docker-compose.yml     # Docker services
â”œâ”€â”€ Makefile              # Build commands
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md             # This file
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

[Add your license information here]

## ğŸ†˜ Support

For issues and questions:
1. Check the troubleshooting section above
2. Review the API documentation at http://localhost:8001/docs
3. Open an issue in the repository

---

**Happy coding! ğŸ‰**
